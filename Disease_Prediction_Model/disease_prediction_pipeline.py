
"""
Disease Prediction Pipeline (tabular, classification)

Usage (quick):
    python disease_prediction_pipeline.py --data "<path/to/data.csv>" --target "<target_column>" --outdir ./runs/heart

Optional arguments:
    --id-cols "id,patient_id"      # comma-separated columns to drop if present
    --positive-label 1             # specify positive class label (default inferred)
    --test-size 0.2
    --models "logreg,svm,rf,xgb"   # choose subset; xgb requires xgboost installed
    --smote                        # enable SMOTE oversampling for training folds
    --calibrate                    # add probability calibration (sigmoid)
    --seed 42

What it does:
- Loads CSV/TSV/XLSX
- Basic EDA summary saved to outdir
- Clean column names, drop ID columns
- Type inference (numeric vs categorical)
- Train/val/test with stratified split + cross-validated model selection
- Models: LogisticRegression, LinearSVC (SVM), RandomForest, XGBoost (if available)
- Metrics: ROC AUC, PR AUC, F1, Accuracy, Recall (Sensitivity), Specificity, Precision
- Curves: ROC, PR; Confusion matrix
- Feature importance: model-based (RF/XGB), coefficients (LogReg), permutation importance
- Saves a best_model.pkl for deployment + a preprocessing pipeline within a single sklearn Pipeline

Author: Generated by ChatGPT
"""
import argparse
import json
import os
import sys
import warnings
from typing import List, Tuple

import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_predict
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,
                             f1_score, recall_score, precision_score, confusion_matrix,
                             RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import joblib

try:
    import xgboost as xgb  # type: ignore
    HAS_XGB = True
except Exception:
    HAS_XGB = False
    warnings.warn("xgboost not installed. Skipping XGB model.", RuntimeWarning)

try:
    from imblearn.pipeline import Pipeline as ImbPipeline  # to support SMOTE inside pipeline cleanly
    from imblearn.over_sampling import SMOTE
    HAS_IMBLEARN = True
except Exception:
    HAS_IMBLEARN = False

warnings.filterwarnings("ignore")

def clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.strip().replace(" ", "_").replace("-", "_").replace(".", "_").lower() for c in df.columns]
    return df

def infer_read(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        try:
            return pd.read_csv(path)
        except Exception:
            return pd.read_csv(path, sep=";")
    if ext in [".tsv"]:
        return pd.read_csv(path, sep="\t")
    if ext in [".xlsx", ".xls"]:
        return pd.read_excel(path)
    raise ValueError(f"Unsupported file type: {ext}")

def summarize_df(df: pd.DataFrame, outdir: str, target: str):
    os.makedirs(outdir, exist_ok=True)
    summary = {
        "rows": int(df.shape[0]),
        "cols": int(df.shape[1]),
        "columns": {col: str(df[col].dtype) for col in df.columns},
        "null_counts": {col: int(df[col].isna().sum()) for col in df.columns},
        "target_balance": df[target].value_counts(dropna=False).to_dict(),
        "target_dtype": str(df[target].dtype),
        "head": df.head(10).to_dict(orient="list")
    }
    with open(os.path.join(outdir, "eda_summary.json"), "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

def split_data(df: pd.DataFrame, target: str, test_size: float, seed: int):
    X = df.drop(columns=[target])
    y = df[target]
    return train_test_split(X, y, test_size=test_size, random_state=seed, stratify=y)

def get_preprocess(X: pd.DataFrame):
    num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]
    cat_cols = [c for c in X.columns if c not in num_cols]
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])
    pre = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ]
    )
    return pre, num_cols, cat_cols

def get_models(args):
    chosen = set([m.strip().lower() for m in args.models.split(",")])
    models = {}
    if "logreg" in chosen:
        models["logreg"] = LogisticRegression(max_iter=1000, class_weight="balanced")
    if "svm" in chosen:
        models["svm"] = LinearSVC(class_weight="balanced", max_iter=5000)
    if "rf" in chosen:
        models["rf"] = RandomForestClassifier(n_estimators=400, random_state=args.seed, class_weight="balanced_subsample")
    if "xgb" in chosen and HAS_XGB:
        models["xgb"] = xgb.XGBClassifier(
            n_estimators=400, learning_rate=0.05, max_depth=5,
            subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,
            random_state=args.seed, n_jobs=-1, eval_metric="logloss",
            scale_pos_weight=None
        )
    return models

def build_pipeline(pre, model, use_smote=False):
    if use_smote and HAS_IMBLEARN:
        return ImbPipeline(steps=[("pre", pre), ("smote", SMOTE()), ("clf", model)])
    return Pipeline(steps=[("pre", pre), ("clf", model)])

def ensure_dir(d):
    os.makedirs(d, exist_ok=True)

def plot_roc_pr(y_true, y_proba, outdir, name):
    if y_proba is None:
        return
    ensure_dir(outdir)
    # ROC
    plt.figure()
    RocCurveDisplay.from_predictions(y_true, y_proba)
    plt.title(f"ROC Curve - {name}")
    plt.savefig(os.path.join(outdir, f"{name}_roc.png"), dpi=200, bbox_inches="tight")
    plt.close()
    # PR
    plt.figure()
    PrecisionRecallDisplay.from_predictions(y_true, y_proba)
    plt.title(f"PR Curve - {name}")
    plt.savefig(os.path.join(outdir, f"{name}_pr.png"), dpi=200, bbox_inches="tight")
    plt.close()

def plot_confmat(y_true, y_pred, outdir, name):
    plt.figure()
    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)
    plt.title(f"Confusion Matrix - {name}")
    plt.savefig(os.path.join(outdir, f"{name}_confusion.png"), dpi=200, bbox_inches="tight")
    plt.close()

def maybe_proba(clf, X):
    # LinearSVC has decision_function, others may have predict_proba
    if hasattr(clf, "predict_proba"):
        return clf.predict_proba(X)[:, 1]
    if hasattr(clf, "decision_function"):
        df = clf.decision_function(X)
        # scale to [0,1] via min-max for PR/ROC consistency
        mn, mx = np.min(df), np.max(df)
        if mx - mn < 1e-12:
            return np.full_like(df, 0.5, dtype=float)
        return (df - mn) / (mx - mn)
    return None

def compute_specificity(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp + 1e-12)

def evaluate_model(name, pipe, X_train, y_train, X_test, y_test, outdir, seed, calibrate=False):
    ensure_dir(outdir)
    # Cross-validated out-of-fold predictions on train for model comparison
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
    oof_pred = cross_val_predict(pipe, X_train, y_train, cv=cv, method="predict")
    if hasattr(pipe, "predict_proba") or hasattr(pipe, "decision_function"):
        oof_proba = cross_val_predict(pipe, X_train, y_train, cv=cv, method="predict_proba" if hasattr(pipe, "predict_proba") else "decision_function")
        if oof_proba.ndim == 2:
            oof_proba = oof_proba[:, 1]
        else:
            # scale decision function
            mn, mx = np.min(oof_proba), np.max(oof_proba)
            oof_proba = (oof_proba - mn) / (mx - mn + 1e-12)
    else:
        oof_proba = None

    # Fit on full train
    pipe.fit(X_train, y_train)

    # Optionally calibrate probabilities
    if calibrate and hasattr(pipe.named_steps["clf"], "predict_proba"):
        from sklearn.calibration import CalibratedClassifierCV
        cal_clf = CalibratedClassifierCV(pipe.named_steps["clf"], cv=3, method="sigmoid")
        cal_pipe = Pipeline(steps=[("pre", pipe.named_steps["pre"]), ("clf", cal_clf)])
        cal_pipe.fit(X_train, y_train)
        pipe = cal_pipe

    # Evaluate on test
    y_pred = pipe.predict(X_test)
    y_proba = maybe_proba(pipe, X_test)

    metrics = {
        "model": name,
        "accuracy": float(accuracy_score(y_test, y_pred)),
        "f1": float(f1_score(y_test, y_pred, zero_division=0)),
        "recall_sensitivity": float(recall_score(y_test, y_pred, zero_division=0)),
        "specificity": float(compute_specificity(y_test, y_pred)),
        "precision_ppv": float(precision_score(y_test, y_pred, zero_division=0)),
    }
    if y_proba is not None:
        metrics["roc_auc"] = float(roc_auc_score(y_test, y_proba))
        metrics["pr_auc"] = float(average_precision_score(y_test, y_proba))

    # Save plots
    plot_confmat(y_test, y_pred, outdir, name)
    plot_roc_pr(y_test, y_proba, outdir, name)

    # Save permutation importance (on test) where feasible
    try:
        r = permutation_importance(pipe, X_test, y_test, n_repeats=10, random_state=seed, scoring="roc_auc" if y_proba is not None else "f1")
        importances = pd.DataFrame({
            "feature": X_test.columns,
            "importance_mean": r.importances_mean,
            "importance_std": r.importances_std
        }).sort_values("importance_mean", ascending=False)
        importances.to_csv(os.path.join(outdir, f"{name}_permutation_importance.csv"), index=False)
    except Exception:
        pass

    # Save the trained pipeline
    joblib.dump(pipe, os.path.join(outdir, f"{name}_best_model.pkl"))

    return metrics

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data", required=True, type=str)
    ap.add_argument("--target", required=True, type=str, help="Target column name")
    ap.add_argument("--outdir", required=True, type=str)
    ap.add_argument("--id-cols", type=str, default="")
    ap.add_argument("--positive-label", type=str, default=None)
    ap.add_argument("--test-size", type=float, default=0.2)
    ap.add_argument("--models", type=str, default="logreg,svm,rf,xgb")
    ap.add_argument("--smote", action="store_true", help="Use SMOTE on training folds (requires imblearn)")
    ap.add_argument("--calibrate", action="store_true")
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)

    df = infer_read(args.data)
    df = clean_columns(df)

    # Drop ID columns if present
    id_cols = [c.strip().lower() for c in args.id_cols.split(",") if c.strip()]
    drop_cols = [c for c in id_cols if c in df.columns]
    if drop_cols:
        df = df.drop(columns=drop_cols)

    target = args.target.strip().lower()
    if target not in df.columns:
        raise SystemExit(f"Target column '{target}' not found. Available: {list(df.columns)[:10]} ... total {len(df.columns)}")

    # Ensure binary labels are 0/1 if possible
    y = df[target].copy()
    if args.positive_label is not None:
        pos_label = args.positive_label
        df[target] = (y.astype(str) == str(pos_label)).astype(int)
    elif y.dtype == "object":
        # map the most frequent class to 0, and the other to 1
        vc = y.value_counts()
        if len(vc) != 2:
            raise SystemExit("Target has more than two classes; this script assumes binary classification.")
        mapping = {vc.index[0]: 0, vc.index[1]: 1}
        df[target] = y.map(mapping)

    # Train/test split
    X_train, X_test, y_train, y_test = split_data(df, target, args.test_size, args.seed)

    # Preprocess
    pre, num_cols, cat_cols = get_preprocess(X_train)

    # Build & evaluate models
    results = []
    for name, model in get_models(args).items():
        pipe = build_pipeline(pre, model, use_smote=args.smote)
        m = evaluate_model(name, pipe, X_train, y_train, X_test, y_test, args.outdir, args.seed, calibrate=args.calibrate)
        results.append(m)

    # Save metrics
    metrics_df = pd.DataFrame(results).sort_values(by=["roc_auc" if "roc_auc" in results[0] else "f1"], ascending=False)
    metrics_df.to_csv(os.path.join(args.outdir, "metrics.csv"), index=False)

    # Pick best model file path (highest roc_auc if available else f1)
    score_key = "roc_auc" if "roc_auc" in metrics_df.columns else "f1"
    best_model_name = metrics_df.iloc[0]["model"]
    best_path = os.path.join(args.outdir, f"{best_model_name}_best_model.pkl")
    with open(os.path.join(args.outdir, "best_model.txt"), "w", encoding="utf-8") as f:
        f.write(best_path)

    # Save column info
    with open(os.path.join(args.outdir, "columns.json"), "w", encoding="utf-8") as f:
        json.dump({"numeric": num_cols, "categorical": cat_cols, "target": target}, f, indent=2)

    print("Done. Results in:", args.outdir)
    print(metrics_df.to_string(index=False))

if __name__ == "__main__":
    main()
